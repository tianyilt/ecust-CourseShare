

# 前五章

数据挖掘：从海量数据中抽取出有用的模式或者知识，这些模式或者知识应该是：==非常识性的、隐藏的、当前未知的以及潜在有益==的。

数据仓库的四个特征：==面向主题的、集成的、时变的、非易失==的数据集合

OLTP(联机事务处理系统)：面向顾客**(用户和系统的面向性)**、管理当前数据**(数据内容)**、ER**(数据库设计)**、不涉及历史数据**(视图)**、由短的原子事务组成**(访问模式)**

OLAP(联机分析处理)：面向市场、管理大量历史数据、星形或雪花模型、处理来自不同单位的信息、大部分都是只读操作

有监督学习：训练数据均含有一个字段，该字段用于表明各个元组所属的类别

无监督学习：训练集的类标号未知

# 分类

## 决策树分类

决策树的生成包括两个过程

​       树的构建

* 首先所有的训练样本都在根结点

* 基于所选的属性循环的划分样本

  树剪枝：识别和删除那些反映噪声或离群点的分支

## 急切学习法 惰性学习法

* 惰性学习（KNN ): 简单存储训练数据 （或只是稍加处理 ) 并且一直等到给定一个检验元组。

优点：可以处理复杂的模式和大规模的训练数据。

缺点：预测可能需要大量的时间和计算资源

* 急切学习 (决策树、神经网络、支持向量机SVM等: 给定训练元组的集合，在收到新的测试数据进行分类之前先构造一个分类器模型

优点：预测速度快，因为目标函数已经在训练阶段就学习好了

缺点：需要大量的时间和计算资源来处理大规模的训练数据

# SVM

* 落在超平面 H 1 或 H 2 即定义边缘的两侧 上的训练元组称为 **支持向量**

* 线性不可分怎么办？转换原始输入数据到一个更高维的空间

## KNN

对于一个需要预测的输入向量x，我们只需要在训练数据集中寻找k个与向量x最近的向量的集合，然后把x的类别预测为这k个样本中类别数最多的那一类。

原理简单、易于理解，可以应用于多分类问题，同时也无需假设数据的分布

## 混淆矩阵

![image-20230615024624055](C:\Users\NP_123\AppData\Roaming\Typora\typora-user-images\image-20230615024624055.png)

$accuracy=\frac{TP+TN}{TP+TN+FP+FN}$

$error=1-accuracy$

$precision=\frac{TP}{TP+FP}$

以猫为例，模型的结果告诉我们，66只动物里有13只是猫，但是其实这13只猫只有10只预测对了。模型认为是猫的13只动物里，有1条狗，两只猪。所以，Precision（猫）= 10/13 = 76.9%

$recall=\frac{TP}{TP+FN}$

precision & recall 之间是相反的关系

以猫为例，在总共18只真猫中，我们的模型认为里面只有10只是猫，剩下的3只是狗，5只都是猪。这5只八成是橘猫，能理解。所以，Recall（猫）= 10/18 = 55.6%

# 聚类

## K-means和K-medoids(划分)

**K-means**和**K-medoids**都是非常常用的**无监督学习**算法，主要用于聚类分析。它们都通过将数据点分组成K个簇，来发现数据集中的结构或模式。

**相同点**：

1. 都是**分区**的聚类算法，将数据点划分为多个不重叠的簇。
2. 都需要**预先设定簇的数量（K）**。
3. 都采用**迭代**的方式优化簇的划分，以最小化簇内点与中心点之间的距离。
4. 在初始阶段，**都会随机选择K个点作为簇的中心点**。

**不同点**：

1. K-means的中心点（称为质心）是簇内所有点的坐标的平均值，**可以不是实际存在的数据**点；而K-medoids的中心点（称为medoid）**是簇内的一个实际存在的数据点**，**这个点到簇内其他点的平均距离最小**。
2. 因为K-medoids的中心点是实际存在的数据点，所以K-medoids比K-means更能抵抗噪声和异常值的影响。

* **K-means的优缺点**：

优点：

1. 复杂度低
2. 相对可伸缩和高效
3. 不能保证得到全局最优解，通常以局部最优解结束。

缺点：

1. 只有在簇的平均值被定义的情况下才能使用，当涉及有分类属性的数据时无法处理
2. 需要事先给出 k ，簇的数目
3. 对噪声和离群点数据敏感
4. 不适合发现非凸形状的簇，或者大小差别很大的簇

* **K-medoids的优缺点**：

优点：

1. 能够更好地处理噪声和异常值。
2. 由于中心点是实际的数据点，**结果更易于解释**。
3. 可以用任何定义在对象对上的距离或相似度函数。

缺点：

1. **计算复杂度高，尤其在大规模数据集上**，计算成本和时间成本都比K-means高。
2. 同样需要==**预先设定K值**==，但实际应用中K值往往是不知道的。

## BIRCH的 特点是什么

增量的构造 CF 树 

1. 优点
   线性伸缩性
   支持增量聚类
2. 缺点
   只能处理数值数据，对数据记录的顺序很敏感

## 层次聚类的优缺点

使用**距离矩阵**作为聚类的标准。这种方法不需要簇的数目 k 作为输入 但需要一个终止条件

**优点**：

1. **可解释性**：层次聚类的结果通常以树状图（或者说是谱系图）的形式呈现，非常直观，易于理解和解释。
2. ==**不需要预设簇的数量**==：不像K-means或K-medoids等分区方法，层次聚类不需要预先设定簇的数量。用户可以通过切割树状图来选择合适的簇的数量。
3. **提供多层次的聚类**：层次聚类提供了不同层次的聚类结果，可以根据需要选择不同的层次。
4. **可以处理任何形状的簇**：层次聚类不像K-means那样假设簇是凸的或者球状的，它可以处理任何形状的簇。

**缺点**：

1. **计算复杂度高**：层次聚类的计算复杂度较高，$O(n^2)以上$，尤其在大规模数据集上。最常用的算法（比如agglomerative层次聚类）的时间复杂度为O(n^2 log(n))，其中n是数据点的数量。
2. ==**对噪声和异常值敏感**==：层次聚类对噪声和异常值比较敏感，这可能会影响聚类的结果。
3. ==**不能撤销**==：一旦层次聚类的过程完成，就无法再调整。例如，如果在某一步合并了两个簇，之后就无法再将它们分开。
4. **质量的一致性**：层次聚类的质量在不同的数据集和不同的应用中可能会有很大的差异，尤其是在选择不同的距离度量和链接方式时。

## 划分聚类

**划分聚类**：

优点：

1. **算法通常比层次聚类更快**，尤其是在大规模数据集上。
2. K-means等划分聚类方法对大规模数据集和高维数据有较好的扩展性。
3. 对于球形或凸形簇，可以得到较好的结果。

缺点：

1. **必须预先指定聚类的数量**。
2. **划分聚类算法通常对初始簇中心的选择敏感，可能会陷入局部最优**。
3. ==**对噪声和异常值敏感。**==
4. **通常假设簇是凸的或球形的**，对于其他形状的簇可能无法得到好的结果。

在选择层次聚类还是划分聚类时，需要考虑数据集的特性（例如大小、维度和簇的形状等），以及具体的应用需求（例如是否需要层次结构、计算成本的承受能力、对噪声和异常值的容忍度等）。

## 基于密度分类

该类方法把簇看作是数据空间中被低密度区域分割开的高密度对象区域。**基于密度的簇是密度相连的点的集合**

- 能够发现任意形状的簇
- 能处理噪声(==优点，对噪声不敏感==)
- 只需一次扫描
- 需要密度参数作为终结条件

基于密度的划分方法：DB-scan（一个基于高密度连接区域的密度聚类方法）

